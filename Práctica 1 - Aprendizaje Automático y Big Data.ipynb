{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1: Regresión Lineal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 : Regresión lineal con una variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Autores: Alberto Pastor Moreno e Iván Fernández Mena**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica uno se va a trabajar la regresion lineal con una variable a partir de los datos almacenados en un archivo csv que se nos proporciona. Los datos extraidos estan separado en dos columnas y representan los beneficios de una compañia de distribución de comida en distintas ciudades en base a su población."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.parsers import read_csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un método que lee y carga los datos de un csv y retorna esta información recopilada. Se dicta que en el archivo no hay información de cabecera (header = None) y que los valores que son devueltos son de tipo flotantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    values = read_csv(filename, header=None).values\n",
    "    return values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_csv('./datasets/ex1data1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el conjunto de datos obtenido del csv con la función especificada previamente y separamos cada columna en vectores diferentes para poder gestionarlos de manera independiente, de este modo de puede usar cada columna para su estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_data = dataset[:,0]\n",
    "dependent_data = dataset[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Visualización simple de dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generará un plotter con los datos que previamente tratados. Se creará un plotter de cruces enfrentando los valores de beneficios de la empresa estudiada y la población que se nos ha proporvionado en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(independent_data, dependent_data, 'rx')\n",
    "plt.ylabel('Ingresos en $10.000s')\n",
    "plt.xlabel('Poblacion de la ciudad en 10.000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Método de descenso gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar la regresión lineal que más se adapte a nuestro dataset, es necesario realizar operaciones básicas de modo que en cada una de las iteraciones se obtenga un resultado más cercano al óptimo. Se necesita definir la función de coste y la función de gradiente de descenso que nos permitira obtener este resultado.\n",
    "\n",
    "Siempre se tiene que tener en cuenta la función que se usará como hipótesis, en nuestro caso se trata de una unica variable y de una recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implementación de la función de hipótesis de la regresión lineal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyphotesis_function(th0, th1, x):\n",
    "    return th0 + th1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implementacion de la funcion de coste de la regresion lineal J($\\theta_0$,$\\theta_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(fun, th0, th1, m, x, y):\n",
    "    sum_cost = 0\n",
    "    for i in range(0, m):\n",
    "        sum_cost += (fun(th0, th1, x[i]) - y[i])**2\n",
    "    cost = sum_cost / (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implementación de función _gradient descent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(fun, th0, th1, m, x, y, lr=0.01, epochs=1500):\n",
    "    cost = []\n",
    "    vc_th0 = []\n",
    "    vc_th1 = []\n",
    "    curr_th0 = th0\n",
    "    curr_th1 = th1\n",
    "    for i in range(0, epochs):\n",
    "        new_th0 = curr_th0 - (lr/m)*np.sum([fun(curr_th0, curr_th1, x[j]) - y[j] for j in range(0, m)])\n",
    "        new_th1 = curr_th1 - (lr/m)*np.sum([((fun(curr_th0, curr_th1, x[j]) - y[j])*x[j]) for j in range(0, m)])\n",
    "        curr_th0 = new_th0\n",
    "        curr_th1 = new_th1\n",
    "        epoch_cost = cost_function(fun, curr_th0, curr_th1, m, x, y)\n",
    "        cost += [epoch_cost]\n",
    "        vc_th0 += [curr_th0]\n",
    "        vc_th1 += [curr_th1]\n",
    "        #print('It: {}, Cost: {}'.format(i, epoch_cost))\n",
    "    return curr_th0, curr_th1, cost, vc_th0, vc_th1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continución se muestran las variables que se han usado para la gestión de la práctica, de este modo se pueden cambiar paramentros de las pruebas de forma comoda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th0 = 0\n",
    "th1 = 0\n",
    "lr = 0.01\n",
    "m = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Resultados obtenidos del estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplican las funciones definidas previamente y se muestran los resultados para poder tomar unas conclusiones concretas. Se ejecuta la función de gradiente de descenso en nuestra función de hipótesis, además de toda la información necesaria gestionada y obtenida de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_th0, gd_th1, gd_cost, vc_gd_th0, vc_gd_th1 = gradient_descent(hyphotesis_function, th0, th1, m, independent_data, dependent_data, lr)\n",
    "print ('th0:{}, th1:{}, cost:{}'.format(gd_th0, gd_th1, gd_cost[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se estudia en los datos mostrados previamente y en la gráfica que se genera posteriormente, se observa que nuestro valor de coste va disminuyendo según se aplica el descenso de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(gd_cost)\n",
    "plt.title('Coste por iteración en el método gradient_descent()')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('num_epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se muestra el resultado de manera coherente enfrentando los valores del dataset original y los datos obtenidos de ejecutar nuestra función gradiente sobre esos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización en plotter normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(independent_data, dependent_data, 'rx', independent_data, hyphotesis_function(gd_th0, gd_th1, independent_data))\n",
    "plt.title('Recta resultado gradient_descent()')\n",
    "plt.ylabel('Ingresos en $10.000s')\n",
    "plt.xlabel('Poblacion de la ciudad en 10.000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una recta que se encuentra ajustada lo máximo posible para nuestro conjunto de datos, esta recta tiene la separación mínima entre los puntos y la resta, obtenido gracias a la función descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización en plotters 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(vc_gd_th0, vc_gd_th1)\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.cos(R)\n",
    "\n",
    "ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "plt.xlabel('Th0')\n",
    "plt.ylabel('Th1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\n",
    "\n",
    "X = np.arange(-10, 10, 0.1)\n",
    "Y = np.arange(-1, 4, 0.1)\n",
    "\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "plt.contour(X, Y, Z ,np.logspace(-2, 3, 20), colors='black')\n",
    "plt.plot(vc_gd_th0[::150], vc_gd_th1[::150], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 : Regresión lineal con dos variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Carga y normalización datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, realizaremos regresión lineal con múltiples variables utilizando un dataset diferente al utilizado en la sección anterior. Por ello, realizamos una nueva carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.104e+03, 3.000e+00, 3.999e+05],\n",
       "       [1.600e+03, 3.000e+00, 3.299e+05],\n",
       "       [2.400e+03, 3.000e+00, 3.690e+05],\n",
       "       [1.416e+03, 2.000e+00, 2.320e+05],\n",
       "       [3.000e+03, 4.000e+00, 5.399e+05]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_csv('./datasets/ex1data2.csv')\n",
    "nfeatures = 2\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset[:,:nfeatures]\n",
    "target = dataset[:,nfeatures]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos los datos de las variables características ya que las unidades utilizadas en el dataset utilizado varían en función al atributo en cuestión. Esto podría generar problemas por lo que sustituimos cada valor de X por su división entre su diferencia con la media de su columna y la desviación estándar de su columna, teniendo en cuenta que cada columna se corresponde con una variable característica en cuestión. \n",
    "\n",
    "Esto queda expresado por: $x_i \\leftarrow \\frac{x_i - \\mu}{\\sigma_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalice(x):\n",
    "    mu = [np.mean(x[:,i]) for i in range(0,len(x[0]))]\n",
    "    sigma = [np.std(x[:,i]) for i in range(0,len(x[0]))]\n",
    "    xnorm = (x - mu)/sigma\n",
    "    return xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.13141542, -0.22609337],\n",
       "       [ 1.        , -0.5096407 , -0.22609337],\n",
       "       [ 1.        ,  0.5079087 , -0.22609337]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_norm, mu, sigma = normalice(features)\n",
    "ones = np.ones([len(features),len(features[0])+1])\n",
    "ones[:,1:] = features_norm\n",
    "features_norm = ones\n",
    "features_norm[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los parámetros que utilizaremos en el cálculo _gradient descent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "th = np.array([1,1,1])\n",
    "lr = 0.01\n",
    "m = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Función hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que tenemos más de una variable característica, en este caso utilizaremos una función definida de la siguiente forma:\n",
    "\n",
    "$ h_\\theta(x)= \\theta^T x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyphotesis_function = lambda th,x : np.dot(x, th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9053220544433592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyphotesis_function(th, features_norm[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Función coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de coste (debajo) queda definida por la siguiente expresión:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^{T}(X\\theta-\\vec{y}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(fun, th, m, x, y):\n",
    "    cost = (x.dot(th) - y).T.dot(x.dot(th) - y)  / (2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65591047222.902596"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function(hyphotesis_function, th, m, features_norm, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 _Gradient descent_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función _gradient descent_ (debajo) actualiza los valores de $\\theta_j$ _epochs_ veces siguiendo la siguiente expresión:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}){x_{j}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(fun, th, m, x, y, lr=0.01, epochs=1500):\n",
    "    cost = []\n",
    "    curr_th = th\n",
    "    for e in range(0, epochs):        \n",
    "        curr_th = curr_th - (1/m)*lr*(np.dot(x.T, fun(th, x) - y))\n",
    "        epoch_cost = cost_function(fun, curr_th, m, x, y)\n",
    "        cost += [epoch_cost]\n",
    "        #print('It: {}, Cost: {}'.format(e + 1, epoch_cost))\n",
    "    return curr_th, cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204247.99574468,  63458.54411537,  32825.3570732 ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_th, gd_cost = gradient_descent(hyphotesis_function, th, m, features_norm, target, lr=0.0004)\n",
    "gd_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 _Normal Equation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma alternativa a _gradient descent_, planteamos la expresión de la ecuación normal que resuelve los valores de $\\theta$ siguiendo la siguiente expresión:\n",
    "\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}\\vec{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89597.90954361   139.21067402 -8738.01911255]\n"
     ]
    }
   ],
   "source": [
    "ones = np.ones([len(features),len(features[0])+1])\n",
    "ones[:,1:] = features\n",
    "features_wones = ones\n",
    "features_t = np.transpose(features_wones)\n",
    "th_norm_eq = np.dot(np.dot(np.linalg.pinv(features_t.dot(features_wones)), features_t), target)\n",
    "print (th_norm_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar que el modelo implementado utilizando _gradient descent_ es correcto, realizamos una predicción con los valores obtenidos de $\\theta$ con nuestra implementación y los valores obtenidos con la ecuación normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción gradient descent:168521.10623406913, predicción ecuación normal:91511.42364428427\n"
     ]
    }
   ],
   "source": [
    "x_test = [1, 1650, 3]\n",
    "x_test[1:] = (np.array(x_test[1:]) - mu ) / sigma\n",
    "print('Predicción gradient descent:{}, predicción ecuación normal:{}'.format(hyphotesis_function(gd_th, x_test),\n",
    "                                                                            hyphotesis_function(th_norm_eq, x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
